{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "edit_CookingLDA_PA_Coursera.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZs3hOE6k49A",
        "colab_type": "text"
      },
      "source": [
        "# Programming Assignment: \n",
        "## Готовим LDA по рецептам"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVOCqYpik49D",
        "colab_type": "text"
      },
      "source": [
        "Как вы уже знаете, в тематическом моделировании делается предположение о том, что для определения тематики порядок слов в документе не важен; об этом гласит гипотеза «мешка слов». Сегодня мы будем работать с несколько нестандартной для тематического моделирования коллекцией, которую можно назвать «мешком ингредиентов», потому что она состоит из рецептов блюд разных кухонь. Тематические модели ищут слова, которые часто вместе встречаются в документах, и составляют из них темы. Мы попробуем применить эту идею к рецептам и найти кулинарные «темы». Эта коллекция хороша тем, что не требует предобработки. Кроме того, эта задача достаточно наглядно иллюстрирует принцип работы тематических моделей.\n",
        "\n",
        "Для выполнения заданий, помимо часто используемых в курсе библиотек, потребуются модули *json* и *gensim*. Первый входит в дистрибутив Anaconda, второй можно поставить командой \n",
        "\n",
        "*pip install gensim*\n",
        "\n",
        "Построение модели занимает некоторое время. На ноутбуке с процессором Intel Core i7 и тактовой частотой 2400 МГц на построение одной модели уходит менее 10 минут."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8vTw-YXk49F",
        "colab_type": "text"
      },
      "source": [
        "### Загрузка данных"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptUmJ88Wk49H",
        "colab_type": "text"
      },
      "source": [
        "Коллекция дана в json-формате: для каждого рецепта известны его id, кухня (cuisine) и список ингредиентов, в него входящих. Загрузить данные можно с помощью модуля json (он входит в дистрибутив Anaconda):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdywZeXHovOO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "outputId": "f59c6f07-5ed5-467c-9af3-952f2842d19b"
      },
      "source": [
        "!pip install scipy==1.2.0\n",
        "!pip install gensim==2.3.0"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scipy==1.2.0\n",
            "  Using cached https://files.pythonhosted.org/packages/67/e6/6d4edaceee6a110ecf6f318482f5229792f143e468b34a631f5a0899f56d/scipy-1.2.0-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scipy==1.2.0) (1.18.5)\n",
            "\u001b[31mERROR: umap-learn 0.4.4 has requirement scipy>=1.3.1, but you'll have scipy 1.2.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.2.0 has requirement scipy==1.4.1; python_version >= \"3\", but you'll have scipy 1.2.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: scipy\n",
            "Successfully installed scipy-1.2.0\n",
            "Processing /root/.cache/pip/wheels/3a/1f/86/63c886325bdffa379a7c91499bc9ea6317a4e4e0fc6e2ff1ce/gensim-2.3.0-cp36-cp36m-linux_x86_64.whl\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim==2.3.0) (1.2.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim==2.3.0) (2.0.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim==2.3.0) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim==2.3.0) (1.18.5)\n",
            "Requirement already satisfied: boto in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim==2.3.0) (2.49.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim==2.3.0) (2.23.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim==2.3.0) (1.14.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim==2.3.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim==2.3.0) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim==2.3.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim==2.3.0) (2020.6.20)\n",
            "Requirement already satisfied: botocore<1.18.0,>=1.17.9 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim==2.3.0) (1.17.9)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim==2.3.0) (0.10.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim==2.3.0) (0.3.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.9->boto3->smart-open>=1.2.1->gensim==2.3.0) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.9->boto3->smart-open>=1.2.1->gensim==2.3.0) (0.15.2)\n",
            "Installing collected packages: gensim\n",
            "Successfully installed gensim-2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaKac6qak49J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93OOvNAAk49T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"recipes.json\") as f:\n",
        "    recipes = json.load(f)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cb39ap2Vk49c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "da7ddb72-1dd0-42f4-d099-1b162334e25d"
      },
      "source": [
        "print(recipes[0])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'id': 10259, 'cuisine': 'greek', 'ingredients': ['romaine lettuce', 'black olives', 'grape tomatoes', 'garlic', 'pepper', 'purple onion', 'seasoning', 'garbanzo beans', 'feta cheese crumbles']}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljPoavotk49n",
        "colab_type": "text"
      },
      "source": [
        "### Составление корпуса"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkhxW9iak49p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "e1bc24c9-b459-44e7-b443-5e922b6898a8"
      },
      "source": [
        "from gensim import corpora, models\n",
        "import numpy as np"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5n_tJOhrk49z",
        "colab_type": "text"
      },
      "source": [
        "Наша коллекция небольшая, и целиком помещается в оперативную память. Gensim может работать с такими данными и не требует их сохранения на диск в специальном формате. Для этого коллекция должна быть представлена в виде списка списков, каждый внутренний список соответствует отдельному документу и состоит из его слов. Пример коллекции из двух документов: \n",
        "\n",
        "[[\"hello\", \"world\"], [\"programming\", \"in\", \"python\"]]\n",
        "\n",
        "Преобразуем наши данные в такой формат, а затем создадим объекты corpus и dictionary, с которыми будет работать модель."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvTwgLxHk490",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "texts = [recipe[\"ingredients\"] for recipe in recipes]\n",
        "dictionary = corpora.Dictionary(texts)   # составляем словарь\n",
        "corpus = [dictionary.doc2bow(text) for text in texts]  # составляем корпус документов"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMAsrMfhk49-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "aa897d4d-c2f7-40a6-9fdd-a04b43b0a298"
      },
      "source": [
        "print(texts[0])\n",
        "print(corpus[0])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['romaine lettuce', 'black olives', 'grape tomatoes', 'garlic', 'pepper', 'purple onion', 'seasoning', 'garbanzo beans', 'feta cheese crumbles']\n",
            "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUktyD95k4-I",
        "colab_type": "text"
      },
      "source": [
        "У объекта dictionary есть полезная переменная dictionary.token2id, позволяющая находить соответствие между ингредиентами и их индексами."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHjvJzkpk4-W",
        "colab_type": "text"
      },
      "source": [
        "### Обучение модели\n",
        "Вам может понадобиться [документация](https://radimrehurek.com/gensim/models/ldamodel.html) LDA в gensim."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dxbt2rvok4-Y",
        "colab_type": "text"
      },
      "source": [
        "__Задание 1.__ Обучите модель LDA с 40 темами, установив количество проходов по коллекции 5 и оставив остальные параметры по умолчанию. \n",
        "\n",
        "\n",
        "Затем вызовите метод модели *show_topics*, указав количество тем 40 и количество токенов 10, и сохраните результат (топы ингредиентов в темах) в отдельную переменную. Если при вызове метода *show_topics* указать параметр *formatted=True*, то топы ингредиентов будет удобно выводить на печать, если *formatted=False*, будет удобно работать со списком программно. Выведите топы на печать, рассмотрите темы, а затем ответьте на вопрос:\n",
        "\n",
        "Сколько раз ингредиенты \"salt\", \"sugar\", \"water\", \"mushrooms\", \"chicken\", \"eggs\" встретились среди топов-10 всех 40 тем? При ответе __не нужно__ учитывать составные ингредиенты, например, \"hot water\".\n",
        "\n",
        "Передайте 6 чисел в функцию save_answers1 и загрузите сгенерированный файл в форму.\n",
        "\n",
        "У gensim нет возможности фиксировать случайное приближение через параметры метода, но библиотека использует numpy для инициализации матриц. Поэтому, по утверждению автора библиотеки, фиксировать случайное приближение нужно командой, которая написана в следующей ячейке. __Перед строкой кода с построением модели обязательно вставляйте указанную строку фиксации random.seed.__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCuFZ-cdk4-a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(76543)\n",
        "# здесь код для построения модели:\n",
        "lda_model = models.LdaModel(corpus=corpus, id2word=dictionary,\n",
        "                            num_topics=40, passes=5)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZ_85x3hk4-j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "res_topics = lda_model.show_topics(num_topics=40, num_words=10, formatted=False)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdkq-E6Vk4-1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_answers1(c_salt, c_sugar, c_water, c_mushrooms, c_chicken, c_eggs):\n",
        "    with open(\"cooking_LDA_pa_task1.txt\", \"w\") as fout:\n",
        "        fout.write(\" \".join([str(el) for el in [c_salt, c_sugar, c_water, c_mushrooms, c_chicken, c_eggs]]))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "uj-Urojkk4-r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "7ce7e7d8-f4d0-48cb-95e7-8a609fc416d1"
      },
      "source": [
        "def define_word_freq_in_topics(word):\n",
        "  return np.sum([[(1 if word_tuple[0] == word else 0) for word_tuple in topic]\n",
        "                 for topic in np.array(res_topics)[:, 1]])\n",
        "  \n",
        "words = [\"salt\", \"sugar\", \"water\", \"mushrooms\", \"chicken\", \"eggs\"]\n",
        "words_freq = list(map(define_word_freq_in_topics, words))\n",
        "\n",
        "print(words_freq)\n",
        "save_answers1(*words_freq)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[19, 8, 10, 0, 1, 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Koa8zikek4_A",
        "colab_type": "text"
      },
      "source": [
        "### Фильтрация словаря\n",
        "В топах тем гораздо чаще встречаются первые три рассмотренных ингредиента, чем последние три. При этом наличие в рецепте курицы, яиц и грибов яснее дает понять, что мы будем готовить, чем наличие соли, сахара и воды. Таким образом, даже в рецептах есть слова, часто встречающиеся в текстах и не несущие смысловой нагрузки, и поэтому их не желательно видеть в темах. Наиболее простой прием борьбы с такими фоновыми элементами — фильтрация словаря по частоте. Обычно словарь фильтруют с двух сторон: убирают очень редкие слова (в целях экономии памяти) и очень частые слова (в целях повышения интерпретируемости тем). Мы уберем только частые слова."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tn2DZtKjk4_D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import copy\n",
        "dictionary2 = copy.deepcopy(dictionary)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fZ9qo2vk4_L",
        "colab_type": "text"
      },
      "source": [
        "__Задание 2.__ У объекта dictionary2 есть переменная *dfs* — это словарь, ключами которого являются id токена, а элементами — число раз, сколько слово встретилось во всей коллекции. Сохраните в отдельный список ингредиенты, которые встретились в коллекции больше 4000 раз. Вызовите метод словаря *filter_tokens*, подав в качестве первого аргумента полученный список популярных ингредиентов. Вычислите две величины: dict_size_before и dict_size_after — размер словаря до и после фильтрации.\n",
        "\n",
        "Затем, используя новый словарь, создайте новый корпус документов, corpus2, по аналогии с тем, как это сделано в начале ноутбука. Вычислите две величины: corpus_size_before и corpus_size_after — суммарное количество ингредиентов в корпусе (для каждого документа вычислите число различных ингредиентов в нем и просуммируйте по всем документам) до и после фильтрации.\n",
        "\n",
        "Передайте величины dict_size_before, dict_size_after, corpus_size_before, corpus_size_after в функцию save_answers2 и загрузите сгенерированный файл в форму."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aS3_XpbKk4_m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_answers2(dict_size_before, dict_size_after, corpus_size_before, corpus_size_after):\n",
        "    with open(\"cooking_LDA_pa_task2.txt\", \"w\") as fout:\n",
        "        fout.write(\" \".join([str(el) for el in [dict_size_before, dict_size_after, corpus_size_before, corpus_size_after]]))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggKZnnT7k4_M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "9c0c5ca0-ef0a-4114-c31c-4df78a4f5e17"
      },
      "source": [
        "# Id ингредиентов, которые встретились более 4000\n",
        "id_to_filter = [key for key, value in dictionary2.dfs.items() if value > 4000]\n",
        "dict_size_before = len(dictionary2.dfs)\n",
        "dictionary2.filter_tokens(bad_ids=id_to_filter)\n",
        "dict_size_after = len(dictionary2.dfs)\n",
        "\n",
        "corpus_size_before = np.sum([len(doc) for doc in corpus])\n",
        "corpus2 = [dictionary2.doc2bow(text) for text in texts]\n",
        "corpus_size_after = np.sum([len(doc) for doc in corpus2])\n",
        "\n",
        "answers2 = [dict_size_before, dict_size_after, corpus_size_before, corpus_size_after]\n",
        "print(answers2)\n",
        "save_answers2(*answers2)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[6714, 6702, 428249, 343665]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hf_gjayRk4_v",
        "colab_type": "text"
      },
      "source": [
        "### Сравнение когерентностей\n",
        "__Задание 3.__ Постройте еще одну модель по корпусу corpus2 и словарю dictionary2, остальные параметры оставьте такими же, как при первом построении модели. Сохраните новую модель в другую переменную (не перезаписывайте предыдущую модель). Не забудьте про фиксирование seed!\n",
        "\n",
        "Затем воспользуйтесь методом *top_topics* модели, чтобы вычислить ее когерентность. Передайте в качестве аргумента соответствующий модели корпус. Метод вернет список кортежей (топ токенов, когерентность), отсортированных по убыванию последней. Вычислите среднюю по всем темам когерентность для каждой из двух моделей и передайте в функцию save_answers3. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hlk6dpek4_x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(76543)\n",
        "# здесь код для построения модели:\n",
        "lda_model2 = models.LdaModel(corpus=corpus2, id2word=dictionary2,\n",
        "                             num_topics=40, passes=5)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ghzzzgsk4_4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "coherences = np.array(lda_model.top_topics(corpus=corpus))[:, 1]\n",
        "coherences2 = np.array(lda_model2.top_topics(corpus=corpus2))[:, 1]"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gywVLfF2k5AI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_answers3(coherence, coherence2):\n",
        "    with open(\"cooking_LDA_pa_task3.txt\", \"w\") as fout:\n",
        "        fout.write(\" \".join([\"%3f\"%el for el in [coherence, coherence2]]))"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsW9tmj1k5AB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "f16a9819-dc22-4b5b-ad0f-34ee67067e26"
      },
      "source": [
        "coherence = np.mean(coherences)\n",
        "coherence2 = np.mean(coherences2)\n",
        "\n",
        "answers3 = [coherence, coherence2]\n",
        "print(answers3)\n",
        "save_answers3(*answers3)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-703.2019367755067, -747.6959617371538]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Twx8AD9ik5AP",
        "colab_type": "text"
      },
      "source": [
        "Считается, что когерентность хорошо соотносится с человеческими оценками интерпретируемости тем. Поэтому на больших текстовых коллекциях когерентность обычно повышается, если убрать фоновую лексику. Однако в нашем случае этого не произошло. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kriNQAR6k5AR",
        "colab_type": "text"
      },
      "source": [
        "### Изучение влияния гиперпараметра alpha"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0utT9gx1k5AS",
        "colab_type": "text"
      },
      "source": [
        "В этом разделе мы будем работать со второй моделью, то есть той, которая построена по сокращенному корпусу. \n",
        "\n",
        "Пока что мы посмотрели только на матрицу темы-слова, теперь давайте посмотрим на матрицу темы-документы. Выведите темы для нулевого (или любого другого) документа из корпуса, воспользовавшись методом *get_document_topics* второй модели:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuBS2wKNk5AU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "e8a4e52e-576c-4cd0-dd95-c59feba3da10"
      },
      "source": [
        "lda_model2.get_document_topics(corpus2)[0]"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(17, 0.128125), (20, 0.12812499999999996), (31, 0.6281249999999858)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCrqc1OFk5Ab",
        "colab_type": "text"
      },
      "source": [
        "Также выведите содержимое переменной *.alpha* второй модели:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIqnRfC9k5Ac",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "outputId": "2444f2c1-5b5d-42df-b118-d98a94c8f17c"
      },
      "source": [
        "lda_model2.alpha"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025,\n",
              "       0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025,\n",
              "       0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025,\n",
              "       0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025,\n",
              "       0.025, 0.025, 0.025, 0.025])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iACZh6oDk5Ap",
        "colab_type": "text"
      },
      "source": [
        "У вас должно получиться, что документ характеризуется небольшим числом тем. Попробуем поменять гиперпараметр alpha, задающий априорное распределение Дирихле для распределений тем в документах."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUpj81Pak5Ar",
        "colab_type": "text"
      },
      "source": [
        "__Задание 4.__ Обучите третью модель: используйте сокращенный корпус (corpus2 и dictionary2) и установите параметр __alpha=1__, passes=5. Не забудьте про фиксацию seed! Выведите темы новой модели для нулевого документа; должно получиться, что распределение над множеством тем практически равномерное. Чтобы убедиться в том, что во второй модели документы описываются гораздо более разреженными распределениями, чем в третьей, посчитайте суммарное количество элементов, __превосходящих 0.01__, в матрицах темы-документы обеих моделей. Другими словами, запросите темы  модели для каждого документа с параметром *minimum_probability=0.01* и просуммируйте число элементов в получаемых массивах. Передайте две суммы (сначала для модели с alpha по умолчанию, затем для модели в alpha=1) в функцию save_answers4."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LviRa-t9k5At",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(76543)\n",
        "# здесь код для построения модели:\n",
        "lda_model3 = models.LdaModel(corpus=corpus2, id2word=dictionary2,\n",
        "                             num_topics=40, passes=5, alpha=1)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nASSBuM1k5A1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "count_model2 = np.sum([len(doc) for doc in lda_model2.get_document_topics(bow=corpus2, minimum_probability=0.01)])\n",
        "count_model3 = np.sum([len(doc) for doc in lda_model3.get_document_topics(bow=corpus2, minimum_probability=0.01)])"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZX8DbLGk5BD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_answers4(count_model2, count_model3):\n",
        "    with open(\"cooking_LDA_pa_task4.txt\", \"w\") as fout:\n",
        "        fout.write(\" \".join([str(el) for el in [count_model2, count_model3]]))"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PfIewDjk5A8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "01eaf80f-1aea-4d57-e928-3aafb2c6f038"
      },
      "source": [
        "answers4 = [count_model2, count_model3]\n",
        "print(answers4)\n",
        "save_answers4(*answers4)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[200750, 1590960]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LD_wEYUQk5BK",
        "colab_type": "text"
      },
      "source": [
        "Таким образом, гиперпараметр __alpha__ влияет на разреженность распределений тем в документах. Аналогично гиперпараметр __eta__ влияет на разреженность распределений слов в темах."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ptj3cvH2k5BM",
        "colab_type": "text"
      },
      "source": [
        "### LDA как способ понижения размерности\n",
        "Иногда, распределения над темами, найденные с помощью LDA, добавляют в матрицу объекты-признаки как дополнительные, семантические, признаки, и это может улучшить качество решения задачи. Для простоты давайте просто обучим классификатор рецептов на кухни на признаках, полученных из LDA, и измерим точность (accuracy).\n",
        "\n",
        "__Задание 5.__ Используйте модель, построенную по сокращенной выборке с alpha по умолчанию (вторую модель). Составьте матрицу $\\Theta = p(t|d)$ вероятностей тем в документах; вы можете использовать тот же метод get_document_topics, а также вектор правильных ответов y (в том же порядке, в котором рецепты идут в переменной recipes). Создайте объект RandomForestClassifier со 100 деревьями, с помощью функции cross_val_score вычислите среднюю accuracy по трем фолдам (перемешивать данные не нужно) и передайте в функцию save_answers5."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ag2hQ5PKk5BO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3qOy98Vk5BV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Подготовим матрицу документы темы\n",
        "teta = np.zeros(shape=(len(corpus2), 40))\n",
        "for i_doc, doc in enumerate(lda_model2.get_document_topics(bow=corpus2)):\n",
        "  for id, prob in doc:\n",
        "    teta[i_doc][id] = prob"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EeG0AdEWk5Bc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = [recipe['cuisine'] for recipe in recipes]"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrMwUPUzk5Bt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_answers5(accuracy):\n",
        "     with open(\"cooking_LDA_pa_task5.txt\", \"w\") as fout:\n",
        "        fout.write(str(accuracy))"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Br7lR9cIk5Bm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clf = RandomForestClassifier(n_estimators=100)\n",
        "scores = cross_val_score(clf, teta, y, cv=3)\n",
        "\n",
        "answers5 = np.mean(scores)\n",
        "save_answers5(answers5)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcUgwf-9k5B1",
        "colab_type": "text"
      },
      "source": [
        "Для такого большого количества классов это неплохая точность. Вы можете попроовать обучать RandomForest на исходной матрице частот слов, имеющей значительно большую размерность, и увидеть, что accuracy увеличивается на 10–15%. Таким образом, LDA собрал не всю, но достаточно большую часть информации из выборки, в матрице низкого ранга."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8dJDjXCk5B4",
        "colab_type": "text"
      },
      "source": [
        "### LDA — вероятностная модель\n",
        "Матричное разложение, использующееся в LDA, интерпретируется как следующий процесс генерации документов.\n",
        "\n",
        "Для документа $d$ длины $n_d$:\n",
        "1. Из априорного распределения Дирихле с параметром alpha сгенерировать распределение над множеством тем: $\\theta_d \\sim Dirichlet(\\alpha)$\n",
        "1. Для каждого слова $w = 1, \\dots, n_d$:\n",
        "    1. Сгенерировать тему из дискретного распределения $t \\sim \\theta_{d}$\n",
        "    1. Сгенерировать слово из дискретного распределения $w \\sim \\phi_{t}$.\n",
        "    \n",
        "Подробнее об этом в [Википедии](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation).\n",
        "\n",
        "В контексте нашей задачи получается, что, используя данный генеративный процесс, можно создавать новые рецепты. Вы можете передать в функцию модель и число ингредиентов и сгенерировать рецепт :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MJKNgfRk5B5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_recipe(model, num_ingredients):\n",
        "    theta = np.random.dirichlet(model.alpha)\n",
        "    for i in range(num_ingredients):\n",
        "        t = np.random.choice(np.arange(model.num_topics), p=theta)\n",
        "        topic = model.show_topic(t, topn=model.num_terms)\n",
        "        topic_distr = [x[1] for x in topic]\n",
        "        terms = [x[0] for x in topic]\n",
        "        w = np.random.choice(terms, p=topic_distr)\n",
        "        print w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1q7tl0Sok5CD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGU3zl9Ik5CK",
        "colab_type": "text"
      },
      "source": [
        "### Интерпретация построенной модели\n",
        "Вы можете рассмотреть топы ингредиентов каждой темы. Большиснтво тем сами по себе похожи на рецепты; в некоторых собираются продукты одного вида, например, свежие фрукты или разные виды сыра.\n",
        "\n",
        "Попробуем эмпирически соотнести наши темы с национальными кухнями (cuisine). Построим матрицу $A$ размера темы $x$ кухни, ее элементы $a_{tc}$ — суммы $p(t|d)$ по всем документам $d$, которые отнесены к кухне $c$. Нормируем матрицу на частоты рецептов по разным кухням, чтобы избежать дисбаланса между кухнями. Следующая функция получает на вход объект модели, объект корпуса и исходные данные и возвращает нормированную матрицу $A$. Ее удобно визуализировать с помощью seaborn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbItU1esk5CL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas\n",
        "import seaborn\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wjr7Lochk5CT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_topic_cuisine_matrix(model, corpus, recipes):\n",
        "    # составляем вектор целевых признаков\n",
        "    targets = list(set([recipe[\"cuisine\"] for recipe in recipes]))\n",
        "    # составляем матрицу\n",
        "    tc_matrix = pandas.DataFrame(data=np.zeros((model.num_topics, len(targets))), columns=targets)\n",
        "    for recipe, bow in zip(recipes, corpus):\n",
        "        recipe_topic = model.get_document_topics(bow)\n",
        "        for t, prob in recipe_topic:\n",
        "            tc_matrix[recipe[\"cuisine\"]][t] += prob\n",
        "    # нормируем матрицу\n",
        "    target_sums = pandas.DataFrame(data=np.zeros((1, len(targets))), columns=targets)\n",
        "    for recipe in recipes:\n",
        "        target_sums[recipe[\"cuisine\"]] += 1\n",
        "    return pandas.DataFrame(tc_matrix.values/target_sums.values, columns=tc_matrix.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BMYHcN7k5Cc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_matrix(tc_matrix):\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    seaborn.heatmap(tc_matrix, square=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cm7X5oo_k5Cn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Визуализируйте матрицу\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Et6XBdxk5Cw",
        "colab_type": "text"
      },
      "source": [
        "Чем темнее квадрат в матрице, тем больше связь этой темы с данной кухней. Мы видим, что у нас есть темы, которые связаны с несколькими кухнями. Такие темы показывают набор ингредиентов, которые популярны в кухнях нескольких народов, то есть указывают на схожесть кухонь этих народов. Некоторые темы распределены по всем кухням равномерно, они показывают наборы продуктов, которые часто используются в кулинарии всех стран. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "um9cwOEAk5Cx",
        "colab_type": "text"
      },
      "source": [
        "Жаль, что в датасете нет названий рецептов, иначе темы было бы проще интерпретировать..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDWMxrV6k5Cz",
        "colab_type": "text"
      },
      "source": [
        "### Заключение\n",
        "В этом задании вы построили несколько моделей LDA, посмотрели, на что влияют гиперпараметры модели и как можно использовать построенную модель. "
      ]
    }
  ]
}